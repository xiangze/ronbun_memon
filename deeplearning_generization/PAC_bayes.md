## PAC-Bayes Compression Bounds So Tight That They Can Explain Generalization
https://openreview.net/forum?id=o8nYuR8ekFm 2022

### TL;DR
:最先端のPAC-Bayes圧縮境界を提案し、それを用いてディープラーニングにおける汎化を理解する。
https://www.arxiv-vanity.com/papers/2211.13609/

code https://github.com/activatedgeek/tight-pac-bayes

### 概要
：ディープニューラルネットワークの非空白(non-vacuous)汎化境界の開発は進んでいるが、これらの境界はディープラーニングがなぜ機能するかについて情報を提供しない傾向にある。本論文では、線形部分空間におけるニューラルネットワークパラメータの量子化に基づく圧縮アプローチを開発し、これまでの結果を大幅に改善し、転移学習を含む様々なタスクにおいて最先端の汎化境界を提供します。これらの厳しい境界を利用して、深層学習における汎化のためのモデルサイズ、等変量、および最適化の暗黙のバイアスの役割をよりよく理解することができる。特筆すべきは、大きなモデルは、これまで知られていたよりもはるかに大きな範囲で圧縮できることであり、オッカムの剃刀を内包していることがわかった。

訓練データの数よりも多くのパラメータがあるにもかかわらず、深層学習モデルは極めてよく汎化し、ランダムなラベルにさえ適合することができます[72]。これらの観測はVC次元やラデマッカー複雑度といった古典的な統計的学習理論では説明できない仮説クラスに対する一様な収束に焦点を当てる[53]。これに対してPAC-Bayesフレームワークは汎化ギャップが*仮説集合全体ではなく学習によって発見された深層学習モデルに依存する*汎化境界を構築する便利な方法を提供する。 
例えば、低スペクトルノルム [57]、 ノイズの安定性 [2]、フラットミニマム [30]、非ランダム化 [55]、頑健性、圧縮 [2, 73]など、訓練データセットによって誘発される深層学習モデルの特性を利用した様々な説明が提案されている。

In this work, we show that neural networks, when paired with structured training datasets, are substantially more compressible than previously known.  Constructing tighter generalization bounds than have been previously achieved

we show that this compression alone is sufficient to explain many generalization properties of neural networks.

特に

- 1. 圧縮されたニューラルネットワークを訓練するために、圧縮サイズを問題の難易度に適応させる新しいアプローチを開発する。パラメータのランダムな線形部分空間[45]で学習し、学習した量子化を行う。パラメータ[45]のランダムな線形部分空間で学習し、学習型量子化を実行する。
	その結果、与えられた精度レベルにおいて、ニューラルネットワークの圧縮サイズを極めて小さくすることができ、これは厳密な境界を設定するために必要不可欠である。を達成することができる。(セクション4参照)。

- 2. 事前符号化オッカムの剃刀と我々の圧縮方式を用いて、画像データセットに対して、データ依存のあるデータセットとないデータセットの両方で、これまでで最も優れた汎化境界を構築する。画像データセットにおいて、データ依存・データ非依存の両プライヤーを用いて、これまでで最も優れた汎化境界を構築する。また、転移学習がどのように圧縮率を向上させるかを示し、その結果 を向上させることを示し、事前学習がもたらす実用的な性能の利点を説明する。(セクション5参照）。

- 3. PAC-Bayesの境界は、事後に対する事前の適応を制約するのみである。データ依存の事前分布を用いた境界の場合 データ依存の事前分布を持つ境界では、事前分布だけで一般化境界と同等の性能を達成することを示す。汎化境界と同等の性能を達成することを示す。したがって、我々は、データ非依存的な事前分布から構築された データ非依存な事前分布を用いた境界は、汎化を理解する上でより有益である。(5.2節参照）。
	
- 4. 圧縮性のレンズを通して、我々は、深層学習モデルがCIFARのような構造化されたデータセットにおいてCIFAR-10のような構造化されたデータセットでは汎化されるが、画素のシャッフルなど構造が破壊された場合、深層学習モデルはCIFAR-10のような構造化されたデータセットで一般化する。同様に、以下の利点も説明します。CNNがMLPを凌駕する理由など、等変数モデルの利点について説明する。最後に、二重降下について調べます。の暗黙の正則化が汎化に必要であるかどうかを検討する(セクション6参照）


### 2 関連作業
PAC ベイズ上限の最適化
		dziugaite2017computing は、バイナリ MNIST 上のディープ確率的ニューラル ネットワークに対する最初の非空の一般化上限を取得しました。著者らは、langford2001境界の緩和を構築し、それを最適化して、SGD を使用して得られた極小値の周囲の大量の低損失解をカバーする事後分布を見つけました。rivasplata2019pac は、blundell2015weightに基づいた PAC ベイズ境界の新しい緩和を開発することで、アイデアをさらに拡張しました。
	モデル圧縮と PAC ベイズ上限
		 小さな摂動に対するニューラル ネットワークの堅牢性に注目し( hinton1993keeper ; hochreiter1997 flat ; langford2001not ; langford2002quantitatively ; keskar2016large ; neyshabur2017pac ; chaudhari2019entropy )、 arora2018stronger は、ノイズ安定性を使用する圧縮ベースのアプローチを開発しました。さらに、彼らはニューラル ネットワークの一般化を研究するために、ランダムな投影を使用して重み行列を再構築する機能を使用しました。続いて、zhou2019nonvacousは、ビット単位の圧縮モデルの表現を使用する PAC-ベイズ上限を開発し、ガウス事後分布とガウス混合事前分布の使用を通じてノイズの安定性を追加しました。さらに、枝刈りと量子化によってさらに小さなモデル表現を実現しました( han2016deep ; cheng2018model )。私たちの圧縮フレームはzhou2019nonvacuousに似ていますが、重要な改良が加えられています。まず、固有次元( Li2018MeasuringTI )と FiLM 部分空間( perez2018film )を使用して、低次元部分空間で学習します。これは剪定よりも効果的で順応性があることが証明されています。次に、可変長コードと量子化を意識したトレーニングを使用した、より積極的な量子化スキームを開発します。最後に、転移学習とデータ依存の事前分布によって提供される圧縮の増加を利用します。
	データ依存の事前分布
	  dziugaite2021data は、 thiemann2017stronglyのような線形 PAC ベイズ境界の場合、データ依存となる事前分布を選択することによって、より厳しい境界を達成できることを実証しました。つまり、事前分布は、保持されたデータの低損失領域の周囲に集中するように訓練されています。より正確には、著者らは、最適なデータ依存事前分布は、トレーニング データのサブセットを与えられた場合の事後分布の条件付き期待値であることを示しています。彼らは、ガウス分布上の変分問題を解くことによって、このデータ依存の事前分布を近似します。彼らは、MNIST、ファッション MNIST、および CIFAR-10 で厳しい境界を取得する、データ依存事前分布に基づいて SGD でトレーニングされたネットワークの上限を評価します。同様に、 perez2021 はデータに依存する事前分布をより緊密に結合します(dziugaite2021data )と PAC-Bayes with Backprop (PBB) ( rivasplata2019pac )を組み合わせて、データ依存の事前分布を使用して MNIST および CIFAR-10 の最先端のPAC-Bayes 非空境界を取得します
	下流への転移学習可能性
	 ding2022pactran は、さまざまな上流モデルの転移可能性を予測するために、PAC ベイズ上限から導出された一般化のさまざまな相関関係を調査します。ただし、目的が異なるため、実際には完全な上限は計算されません。
	私たちの焦点は、ディープ ニューラル ネットワークにおける一般化をよりよく理解するために、より良い境界を達成することです。たとえば、境界に対する転移学習、等分散、確率的トレーニングの効果を調査し、一般化を説明する際のデータに依存しない境界の重要性を主張します。以前の結果と比較した限界の改善を表 1にまとめます。

### 3 A Primer on PAC-Bayes Bounds

　　PAC-Bayes bounds are fundamentally an expression of Occam’s razor: simpler descriptions of the data generalize better.

PAC-Bayes境界は、基本的にオッカムの剃刀を表現したもので、データの記述が単純なほどよく一般化されます。例として、有限の仮説クラスに対する古典的な汎化境界を考えてみましょう。ここで 
^R(h) 1n∑ni=1ℓ(h(xi),yi) は 仮説の経験的リスクである h∈Hである。|H|<∞. とする。ℓとする。0-1 の損失とし R(h)=E[^R(h)] は人口リスクを表す。少なくとも確率で 1-δの確率で，仮説の母集団リスク h を使って n を満たす。

R(h) ≤ Rˆ(h) + r log |H| + log(1/δ) 2n .     (1)

つまり、人口リスクは、経験的リスクと複雑な項によって境界を定められる。log det(H) という複雑性の項があり、これは任意の仮説を指定するのに必要なビット数を数えるh∈H.しかし、各仮説が等しくあり得ると信じていない場合はどうだろうか。仮説クラスの事前分布が、可能性の高い仮説に集中していると考えれば、より少ないビット数でそれらの仮説を指定する可変長符号を構成することができる。なお、もしP に対する事前分布であるHに対する事前分布であるとすると、与えられた仮説h はlog_2 1/P(h) の最適な圧縮符号を使用する場合に表現するビット数。P.仮説クラスの大きさに関係なく、データと一致する仮説がこの事前分布のもとでもありそうであれば、この事前分布は複雑さの項をより小さくすることができる。
さらに、必要なビット数は、以下のように減らすことができる。
	log_2(1/P(h)) へのKL(Q∥P) を考えることで、「良い」解の分布Q.のどの要素かを気にしないのであればQ のどの要素にたどり着くかを気にしないのであれば，この鈍感さからいくつかのビットを取り戻すことができます（これは別のメッセージを符号化するために使うことができます）．のサンプルを符号化するのに必要な平均ビット数は以下の通りです。
Q を符号化するのに必要な平均ビット数は、事前定義P はクロスエントロピーH(Q,P) となり、次のようになります。
H(Q) は，どのサンプルについて不可知論であることから，ビットバックされる
h∼Q の間のKL-ダイバージェンスが得られます。Q とP:H(Q,P)-H(Q)=KL(Q∥P).
有限仮説境界のこれらの改善により、以下のように置き換えることができます。
\を置き換えるとH をKL(Q∥P)という仮説を立て、サンプリングする。h∈H - をサンプリングすると、mcallester1999pacで紹介されたPAC-Bayesの束縛に(細かい帳尻合わせをしながら)到達する。この最後の束縛は、少なくとも以下の確率で1-δ,

E h∼Q [R (h)] ≤ E h∼Q [Rˆ (h)] + sqrt {  KL(Q ∥ P) + log(n/δ) + 2/ 2n − 1 }


### 4 . Tighter Generalization Bounds via Adaptive Subspace Compression

ニューラルネットワークの学習は、高次元空間で多くの勾配ステップを踏む必要があります。
\数値D.しかしD は大きいかもしれないが、損失ランドスケープは一般的に考えられているよりも単純であることが分かっている（Dauphin2014IdentifyingAA; Goodfellow2015QualitativelyCN; Garipov2018LossSM）。より一般的な固有次元の概念に類似して、Li2018MeasuringTIは、ネットワークを訓練してなお訓練データに適合することができる最も低い次元の部分空間を探索しました。ニューラルネットワークの重み
θ∈\の重みはD の重みは初期化によってパラメトリック化される。θ0 と投影w∈\はd を固定行列を介して低次元部分空間へ投影する。P∈\をD×d,
θ=θ0+Pw. (3)
最適化時のコンディショニングを有利に進めるためP はほぼ正則(orthogonal)となるように選択される
P⊤P≈Id×d.スケーラビリティのため、Li2018MeasuringTIでは、以下の形式のランダム正規行列を使用します。
P∼\ガウシアン0,1D×d/√Dと、そのスパース近似（Li2006VerySR; Le2013FastfoodAK） があります。

本来の姿である固有次元（ID）は、学習課題の複雑さを測定するための科学的なツールとしてのみ使用される。枝刈りのような方法とは異なり、固有次元（Li2018MeasuringTI）はタスクの複雑さに応じてスケールし、より複雑なタスクにはより大きな固有次元が必要です。その後、IDを量子化と組み合わせることで、効果的なモデル圧縮方法として機能することが発見された。モデルの固有次元に似た考え方は、上限を推定するためのモデル圧縮の文脈で探求されてきたことに留意せよ（arora2018stronger）。我々の仕事では、固有次元d≪D を求める能力は、モデルの圧縮性、ひいては汎化境界を構築する能力に対して深い意味を持つことがわかった。zhou2019nonvacuousによって実証されたように**ニューラルネットワークの圧縮性は汎化に直結しており**、転移学習の非空間PAC-Bayes境界を計算することができるのです。
IDに基づき、厳しい汎化境界を達成するための我々の主要な構成要素は、
	(i)式3でパラメータ化された固有次元のニューラルネットワークを訓練するスケーラブルな新しい方法（4.1節）
	(ii)最大圧縮のための量子化ニューラルネットワーク重みと量子化レベルの両方を同時に訓練する新しいアプローチ（4.2節）
	
	からなる。我々の完全な方法はアルゴリズム1に要約されている。

アルゴリズム1

#### 4.1 Finding Better Random Subspaces


### 5 Empirical Non-Vacuous Bounds


## Sharp Minima Can Generalize For Deep Nets
https://dl.acm.org/doi/pdf/10.1145/3446776

https://arxiv.org/abs/1703.04933
	https://arxiv.org/abs/1703.04933
	https://github.com/arXivTimes/arXivTimes/issues/248
> DNNはすごく過学習しやすそうなのになぜ一般解(と思われるもの)を獲得できるのかについて、「最適解近辺がフラットだから」という予測があった。しかし挙動が変化しない類のパラメーター変換でも誤差平面の平坦さは影響を受けることが分かった。つまり、平坦の定義も含め議論の余地ありという話。


## Stronger generalization bounds for deep nets via a compression approach　(Arora2018)

https://arxiv.org/abs/1802.05296
2018年 Sanjeev Arora, Rong Ge,  Behnam Neyshabur,  Yi Zhang

DNN、訓練サンプルの数よりも多くのパラメータを持つにもかかわらず、うまく汎化する。 最近の研究では、PAC-BayesやMargin-based analysisを用いて説明を試みているが、素朴なパラメータカウントよりも優れたサンプル複雑度の上限を得るには至っていない。
本論文では、それより何桁も優れた汎化上限を示す。これは、学習済みネットの新しい簡潔なリパラメトリゼーションに依存しており、明示的かつ効率的な圧縮が可能である。この圧縮は、ここで紹介するシンプルな圧縮ベースのフレームワークにより、汎化上限を得ることができる。
また、我々の結果は、ディープネットの圧縮が経験的に広く成功していることの理論的な正当性を示すものである。
この圧縮の正しさの解析は、学習済みディープネットの「**ノイズ安定性**」特性を新たに特定し、実験的に検証したものである。 また、これらの性質とその結果得られる汎化境界の研究は、汎化を証明する以前の試みから逃れていた畳み込みネットにも拡張されている。

DNNの汎化上限は、機械学習における広範な研究対象である。汎化境界の改善で期待されているアプローチの1つに、圧縮アプローチがある。圧縮アプローチは、ディープネットワークのパラメータや活性度を記述または圧縮するために必要な情報量を定量化することで、ディープネットワークの複雑さを測定することを目的としています。

圧縮アプローチの基本的な考え方は、ディープネットワークが情報を大幅に失うことなくコンパクトな表現に圧縮できるのであれば、それはデータ中の本質的なパターンと構造を捉えているはずだというものです。言い換えれば、そのネットワークはオーバーフィットしておらず、優れた汎化能力を持っているということです。

圧縮アプローチを用いることで、研究者はディープネットワークのより強力な汎化境界を導きました。これらの境界は、通常、圧縮された表現の複雑さ指標に依存する。複雑さの尺度には、非ゼロパラメータの数、パラメータや活性化のエントロピー、ネットワークの有効次元数などが含まれることがある。

圧縮の枠組みにおける一般的なアプローチの1つは、「パッキング数」法と呼ばれるものである。パッキング数法は、与えられた入力に対してネットワークが生成できる異なる出力の数を定量化することで、仮説クラス（ネットワークが表現できる関数の集合）の複雑さを測定する方法を提供します。パッキング数を制限することで、ディープネットワークの汎化上限をより厳しくすることができる。

より強力な汎化境界を導き出すために使用されてきたもう一つのアプローチは、アルゴリズムの安定性の概念に基づくものである。アルゴリズムの安定性は、入力データの小さな摂動に対するネットワークの予測値の頑健性を測定するものです。ディープネットワークの安定性を分析することで、研究者は汎化境界を改善することができるようになりました。

圧縮アプローチは、汎化境界の改善において有望であるが、まだ活発な研究分野であり、万能な解決策はないことに留意する必要がある。圧縮アプローチの有効性は、特定のネットワークアーキテクチャ、データセット、および学習タスクによって異なる可能性があります。したがって、ディープニューラルネットワークの汎化を改善するための圧縮アプローチの可能性を完全に探るには、さらなる研究と実験が必要である。

## Generalization Error Bounds for Noisy, Iterative Algorithms
https://arxiv.org/abs/1801.04295
### 概要
統計的学習理論において、汎化誤差は教師あり機械学習アルゴリズムが訓練データに過適合する可能性がある程度を定量化するために使用される。最近の研究［Xu and Raginsky (2017)］は、損失関数がサブガウス型である場合に、アルゴリズム入力Sとアルゴリズム出力Wの間の相互情報量I (S; W)に基づく経験的リスク最小化の一般化誤差の境界を確立した。我々は、これらの結果を利用して、マルコフ構造を持つ有界でノイズの多い更新を特徴とする幅広いクラスの反復アルゴリズムに対する一般化誤差の境界を明らかにする。我々の境界は非常に一般的であり、確率勾配ランジュバン力学（SGLD）や確率勾配ハミルトニアンモンテカルロ（SGHMC）アルゴリズムの変種を含む多くの興味深い設定に適用可能である。さらに、我々の誤差境界は、アルゴリズムの最後のイテレートやイテレートのサブセットの平均を含む、イテレートの経路上で計算された任意の出力関数に対して成立し、アルゴリズムの連続更新におけるデータの非均一サンプリングも許容する。

多くの一般的な機械学習アプリケーションは、経験的リスク最小化（ERM）[15,18]の枠組みの中で行うことができる。このリスクは、適切な損失関数の期待値として定義され、その期待値は母集団に渡って取られる。ERMではリスクを直接最小化するのではなく、学習セットに含まれるデータポイントの有限サンプルで評価した損失関数の経験的平均を最小化することで進める[16]。ERM問題の計算効率が良く、最適に近い解を得ることに加え、経験的リスクが損失関数の真のリスクからどの程度乖離しているかを定量化する必要があるため、ERM推定値がデータ生成分布の基礎パラメータにどの程度近いかを決定する。

本論文では、反復型ERMアルゴリズムのファミリーに焦点を当て、そのようなアルゴリズムから得られるパラメータ推定値の一般化誤差の境界を導出する。本論文で考察する反復アルゴリズムの統一的な特徴は、連続する各更新にノイズの追加が含まれ、学習アルゴリズムが訓練データに過剰適合することを防ぐことである。さらに、アルゴリズムの反復はマルコフ構造を介して関連し、連続する更新の間の差（ノイズ項を無視する）は有界であると仮定される。この種の学習アルゴリズムとしては、確率的勾配ランジュバン力学（SGLD）が有名であるが、これは確率的勾配降下法（SGD）の一種で、各反復でガウスノイズを注入し勾配が拘束された損失関数に適用するものと見なすことができる。我々のアプローチは、入力データセットと出力パラメータ推定値の間の相互情報を用いて汎化誤差を制限する最近の結果 [14, 20] を活用する。重要なのは、この手法により、相互情報の連鎖法則を適用することができ、アルゴリズムの反復の任意の関数として得られる推定値に拡張する単純な分析につながることである。また、サンプリング戦略は、データに依存し、時間的に変化することが許されるが、パラメータには不可知論であるべきである。
SGDの一般化特性は、最近、**アルゴリズムの安定性**を含む異なるアプローチを用いて導出された[6,8]。その主な考え方は、1つのデータポイントの追加や削除によって、小さな有界量で変化する学習アルゴリズムも、かなりうまく汎化しなければならないというものである[2,4,10]。
しかし、SGDが安定なアルゴリズムであることを示すために採用された議論は、更新が有界勾配ステップを使用して得られるという事実に決定的に依存している。Mouら[9]は、安定性とHellinger距離の二乗を関連付け、後者の量を制限することにより、SGLDの一般化誤差の境界を提供している。彼らの一般化誤差境界は、ある場合には我々のものより厳しいが、純粋に情報理論的な安定性の概念（すなわち、相互情報）に基づく我々のアプローチは、反復の平均を含む、より一般的な更新と最終出力のクラスを考慮することができる。さらに、我々の枠組みで分析したアルゴリズムは、訓練データセット上の非均一サンプリングスキームに関連して反復更新を行う場合がある。

#### Contribution

1. 一般化限界を証明するための単純な圧縮フレームワーク(セクション 2 )。おそらく PAC-Bayes の作業のより明示的かつ直観的な形式です。また、最近の一般化結果の基本的な短い証明も得られます (セクション 2.2 )。
2. ディープ ネットの新しい形式のノイズ安定性の特定: 下位層で注入されるノイズに対する各層の計算の安定性。(以前の論文では、出力層の安定性のみを考慮していました。) 図 1は、ガウス注入ノイズに関するネットワークの安定性を視覚化しています。正式なステートメントには、他のプロパティの文字列が必要です (セクション 3 )。すべては、一般化との相関関係を含め、経験的に研究されています (セクション 6 )。
3. 上記のプロパティを使用して、ネット内のパラメータの有効数を減らす、効率的で証明可能で正しいアルゴリズムを導き出すと、次のような一般化限界が得られます。 (a) 単純なパラメータのカウント (セクション 6) よりも優れています (b) シンプルで直感的かつ測定可能なものに依存 します。ネットワークの特性 (セクション 4 ) (c) は畳み込みネットにも適用されます (セクション 5 ) (d) 一般化と経験的に相関します (セクション 6 )。

Non-Vacuous Generalization Bounds at the ImageNet Scale: A PAC-Bayesian Compression Approach
https://arxiv.org/abs/1804.05862
https://www.cs.princeton.edu/~rpa/pubs/zhou2019nonvacuous.pdf

現代のニューラルネットワークは、非常にオーバーパラメータであり、学習データに対して大幅に過学習する可能性を備えています。しかし、これらのネットワークは実際にはよく汎化されることが多い。また、訓練されたネットワークは、しばしば、より小さな表現に「圧縮」することができることが観察されている。本論文の目的は、この2つの経験的観測を結びつけることである。我々の主な技術的成果は、圧縮サイズに基づく圧縮ネットワークの汎化境界である。特に、ImageNetの分類問題に適用される現実的なアーキテクチャに対して、初めて非空間的な一般化保証を提供するものである。圧縮と汎化を結びつける追加の証拠として、オーバーフィットの傾向があるモデルの圧縮性が制限されることを示す： 期待される汎化誤差の関数として、期待される圧縮性の絶対的な限界値を確立する。この限界は、オーバーフィットの増加は、学習済みネットワークを記述するのに必要なビット数の増加を意味することを示す経験的結果によって補完される。


## Tightening Mutual Information Based Bounds on Generalization Error
https://arxiv.org/pdf/1901.04609.pdf

教師あり学習アルゴリズムの汎化誤差に対する情報理論的な上界が導かれる。各個の訓練サンプルと学習アルゴリズムの出力との間の相互情報で構成される。この境界は、既存の研究よりも、より一般的な損失関数の条件の下で導出される にもかかわらず、汎化誤差をより厳密に特徴付けることができる。は汎化誤差をより厳密に特徴付けることができる。
また、学習アルゴリズムの例を示し、この境界の厳密さを示すとともに、広範に適用可能であることを示す。確率的勾配ランジュバン力学(SGLD)などのノイズの多い反復アルゴリズムへの応用も検討する、 ここで、構築された境界は、既存の結果よりも一般化誤差をより厳密に特徴づける 般化誤差を既存の結果よりも厳密に定義することができる。最後に、以下のことが示される。一般化誤差をより厳密に特徴付けることができる。経験的に評価することが難しい既存の境界とは異なり、提案された境界は実際に容易に推定できることが実証された。

## 平均場近似に基づいた非空の汎化限界の分析

https://arxiv.org/abs/1909.03009
コンスタンティノス・ピタス

オーバーパラメータ化されたニューラル ネットワークが、ベンチマーク データセットで低リスクとゼロ経験的リスクを同時に実現する方法を説明することは、未解決の問題です。
変分推論(VI)を使用して最適化された PACベイズ境界は、非空限界(Non-Vacuous Bounds)を取得する際の有望な方向として最近提案されています。我々は、平均場近似として知られる、対角共分散を伴うガウスとして事後分布をモデル化する場合、このアプローチでは無視できるゲインしか得られないことを経験的に示します。

最適化の問題や次善の事前分布の選択による VI の失敗など、一般的な説明を調査します。私たちの結果は、より豊かな事後分布を調査することが今後の最も有望な方向性であることを示唆しています。

->

## On PAC-Bayes Bounds for Deep Neural Networks using the Loss Curvature

https://openreview.net/forum?id=SklgfkSFPH

私たちは、トレーニング損失のヘッセ行列を最小限に活用することで、DNNのPACベイズ境界を厳しくすることが可能かどうかを調査します。ガウス事前分布と事後分布の場合、層ごとの部分問題の閉じた形式の解に依存するより厳密な PAC ベイズ境界を取得するヘシアン ベースの方法を導入します。最新のディープ アーキテクチャでは実装が難しく、時間がかかる可能性がある一般的に使用される変分推論手法を回避します。慎重な実験を通じて、より厳密な境界の取得に対する事前平均、事前共分散、事後平均、事後共分散の影響を分析します。より有益な事前分布を通じて PAC ベイズ境界をさらに改善する場合のいくつかの制限について説明します。


