https://arxiv.org/abs/1712.09913

論文調査　https://www.coyote009.com/loss-landscape
https://www.coyote009.com/wp-content/uploads/2022/03/li2018_visualizing_loss_landscape.pdf
- 
例えば、Loss関数のFlatness/Sharpnessと、汎化性能に相関があるというこれまでの主張は本 当か？
既存の取組み(1) • 重み空間中の2点間を結ぶ直線上のLoss関数のプロファイルをとる [Goodfellowなど]



# 関連情報

「ニューラル ネットワーク最適化問題の質的特徴付け」の再検討 arxiv.org/abs/2012.06898 
2014年のGoodfellowらによる当時のよく分類できるネットワークの「目的関数は単純でほぼ凸形状をしている」という性質はMNISTより難しいデータセットと最先端のネットワークではなりたたないのではという指摘

## ref
https://medium.com/mlearning-ai/visualising-the-loss-landscape-3a7bfa1c6fdf
notebook付き

## pytorch実装
https://github.com/xiangze/loss-landscape

## 被引用リスト
https://scholar.google.co.jp/scholar?q=Visualizing+the+Loss+Landscape+of+Neural+Nets&hl=ja&as_sdt=0&as_vis=1&oi=scholart
https://www.google.com/search?q=Visualizing+the+Loss+Landscape+of+Neural+Nets&sourceid=chrome&ie=UTF-8

https://losslandscape.com/ LANDSCAPE


## 関係ありそうな論文
[1710.05468] Generalization in Deep Learning
	   arXiv:1710.05468 - Google 検索  1710.05468.pdf arXiv:1710.05468 - Google 検索
AI、機械学習、深層学習に関する arXiv.org のベスト – 2020 年 6 月 - insideBIGDATA

		
[1703.04933] Sharp Minima Can Generalize For Deep Nets
		Sharp Minima Can Generalize For Deep Nets

[1412.6544] Qualitatively characterizing neural network optimization problems
		[2012.06898] 「ニューラル ネットワーク最適化問題の質的特徴付け」の再検討
		2012.06898.pdf

[1612.04010] An empirical analysis of the optimization of deep network loss surfaces
		1612.04010.pdf
		
Topology of Learning in Artificial Neural Networks 1902.08160.pdf
Topology of Learning in Artificial Neural Networks - Google 検索
		Topology of Learning in Artificial Neural Networks - Google 検索
		[1902.08160] Topology of Learning in Artificial Neural Networks

