The Implicit Regularization of Dynamical Stability in Stochastic Gradient Descent

https://arxiv.org/abs/2305.17490

**動的安定性**というアイデアを用いた確率的勾配降下法(SGD)の暗黙的な正則化の研究

まずSGD の既存の安定性分析(Wu et al., 2018)を改訂し、フロベニウスノルムとヘシアンのトレースが安定性のさまざまな概念にどのように関連しているかを示します。
特に、大域的最小値がSGDに対して線形に安定している場合、ヘッセ行列のトレースは2/η以下でなければなりません(ηは学習率)。
対照的に、勾配降下法(GD)の場合、安定性は同様の制約を課しますが、ヘッセ行列の最大固有値にのみ課されます。

次に、特に2層ReLUネットワークと対角線形ネットワークに焦点を当てて、これらの安定した最小値の一般化特性の分析を行う。
特に、これらの鮮鋭度の指標と2つのモデルの特定のパラメーター規範の間に**等価性**を確立します。これによりSGDの安定した最小値がうまく汎化できることが証明できます。対照的に安定性に起因するGDの正則化は、満足のいく一般化を保証するには弱すぎることが証明されています。
この矛盾は、SGDがGDよりもよく汎化できる理由を説明します。学習率(LR)は、安定性による正則化の強度において極めて重要な役割を果たすことに注意。LRが上がると、正則化効果はより顕著になりLR が大きいSGDが一貫して優れた汎化能力を示す理由が解明される。
さらに、理論的発見を裏付けるために数値実験も提供されています。
