* 深層学習の原理に迫る
https://www.iwanami.co.jp/book/b570597.html

縦書き100ページくらいで参考文献は10個くらいですがその分密度が非常に高いです。
キーワード、関連情報の抽出と個人的感想、参考文献リストです。

====

$$ \exp$$

##  1 深層学習の登場

##  2 深層学習とは何か

## 3なぜ多層が必要化
このへんから本番
万能近似

## 4 膨大なパラメーター数の謎

”宝くじ仮説①”　
”宝くじ仮説②”　値が初期値から少ししか動いていない
"平坦仮説"
 PACベイズ理論

##  5 なぜパラメーターの学習ができる？

## 6 原理を知ることに価値はあるか

## 参考文献
本書では提示されてなかった3つの仮説関連の論文の補完を試みます
### 3つの仮説
制約されたパラメーター集合
- 宝くじ仮説①
パラメーターの値がそもそも小さい。　初期値はall 0?
- 宝くじ仮説②
パラメーターの探索範囲が狭い　初期値に依存する

- 平坦仮説

 - PACベイズ理論

### その他

- 平均場理論
唐木田 亮先生の参考文献から [https://sites.google.com/view/ismstatphys/:title]
[https://arxiv.org/abs/2205.06798:title]
[https://www.semanticscholar.org/paper/Spectrum-Dependent-Learning-Curves-in-Kernel-and-Bordelon-Canatar/77a65e190f0815a6ca4755c1f7bc0d71bf4ebfe2:title]
[https://arxiv.org/abs/2112.01653:embed:cite]

Random matrix analysis of deep neural network weight matrices
スピングラスとの関係 
数理科学2020/11

