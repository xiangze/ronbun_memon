# Comparing Dynamics: Deep Neural Networks versus Glassy System
https://proceedings.mlr.press/v80/baity-jesi18a/baity-jesi18a.pdf
https://iopscience.iop.org/article/10.1088/1742-5468/ab3281

We analyze numerically the training dynamics of deep neural networks (DNN) by using methods developed in statistical physics of glassy systems. 

The two main issues we address are 
- (1) the complexity of the loss landscape and of the dynamics within it, and 
- (2) to what extent DNNs share similarities with glassy systems. 
  
Our findings, obtained for different architectures and datasets, suggest that during the training process the dynamics slows down because of an increasingly large number of flat directions. At large times, when the loss is approaching zero, the system diffuses at the bottom of the landscape. Despite some similarities with the dynamics of mean-field glassy systems, in particular, the absence of barrier crossing, we find distinctive dynamical behaviors in the two cases, showing that the statistical properties of the corresponding loss and energy landscapes are different. In contrast, when the network is under-parametrized we observe a typical glassy behavior, thus suggesting the existence of different phases depending on whether the network is under-parametrized or over-parametrized.

異なるアーキテクチャとデータセットで得られた我々の知見は、学習プロセスにおいて、平坦な方向がますます多くなるため、ダイナミクスが遅くなることを示唆している。
損失がゼロに近づくような大きな時間帯では、システムはランドスケープの底で拡散する。
平均場ガラス系のダイナミクスとの共通点、特にバリアクロスがないことなどが挙げられるが、2つのケースで特徴的な力学的挙動を見出し、対応する損失とエネルギーランドスケープの統計的性質が異なることが示された。
一方、ネットワークがアンダーパラメトリー化された場合は、典型的なガラスのような挙動を示し、アンダーパラメトリー化かオーバーパラメトリー化かによって異なる相が存在することが示唆された。



